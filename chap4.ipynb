{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chap4.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LCfDlKsLhhiM","colab_type":"text"},"source":["<h3>7. Algorithm Design </h3>"]},{"cell_type":"markdown","metadata":{"id":"I7-RGFCVhxUf","colab_type":"text"},"source":["A major part of algorithmic problem solving is selecting or adapting an appropriate algorithm for the problem at hand. The best known strategy is known as divide-and-conquer. We attack a problem of size n by dividing it into two problems of size n/2, solve these problems, and combine their results into a solution of the original problem. "]},{"cell_type":"markdown","metadata":{"id":"ujIo4CAskWRg","colab_type":"text"},"source":["**Divide-and-Conquer**"]},{"cell_type":"markdown","metadata":{"id":"lGtdRIM4kfgT","colab_type":"text"},"source":["Sorting by Divide-and-Conquer: To sort an array, we split it in half and sort each half (recursively); we merge each sorted half back into a whole list (again recursively); this algorithm is known as \"Merge Sort\"."]},{"cell_type":"markdown","metadata":{"id":"m8Eq44tLkwld","colab_type":"text"},"source":["![alt text](https://scontent.fist2-4.fna.fbcdn.net/v/t1.15752-9/64789182_2601548716523414_7838641960347762688_n.png?_nc_cat=111&_nc_ht=scontent.fist2-4.fna&oh=816cf0bb633ee359979ea0271ed390b2&oe=5D8F60B7)"]},{"cell_type":"markdown","metadata":{"id":"aZ-3BZ-1lsnz","colab_type":"text"},"source":["**Binary Search**"]},{"cell_type":"markdown","metadata":{"id":"E72UGxLhlyuW","colab_type":"text"},"source":["Another example is the process of looking up a word in a dictionary. We open the book somewhere around the middle and compare our word with the current page. If it's earlier in the dictionary we repeat the process on the first half; if its later we use the second half. This search method is called binary search since it splits the problem in half at every step."]},{"cell_type":"markdown","metadata":{"id":"AE7R_Goel4ZT","colab_type":"text"},"source":["**Recursion**"]},{"cell_type":"markdown","metadata":{"id":"aUbcRGhvl-Co","colab_type":"text"},"source":["We define a function f which simplifies the problem, and calls itself to solve one or more easier instances of the same problem. It then combines the results into a solution for the original problem."]},{"cell_type":"markdown","metadata":{"id":"xxfjmo2Wq81N","colab_type":"text"},"source":["**Example**"]},{"cell_type":"markdown","metadata":{"id":"Ean2V8mWq_Sa","colab_type":"text"},"source":["For example, suppose we have a set of n words, and want to calculate how many different ways they can be combined to make a sequence of words. If we have only one word (n=1), there is just one way to make it into a sequence. If we have a set of two words, there are two ways to put them into a sequence. For three words there are six possibilities. In general, for n words, there are n × n-1 × … × 2 × 1 ways (i.e. the factorial of n). We can code this up as follows:"]},{"cell_type":"code","metadata":{"id":"29xZBV_brCGd","colab_type":"code","colab":{}},"source":["def factorial1(n):\n","     result = 1\n","     for i in range(n):\n","      result *= (i+1)\n","      return result"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cKdwX9Hrr8fL","colab_type":"text"},"source":["Suppose we have a way to construct all orderings for n-1 distinct words. Then for each such ordering, there are n places where we can insert a new word: at the start, the end, or any of the n-2 boundaries between the words. Thus we simply multiply the number of solutions found for n-1 by the value of n. We also need the base case, to say that if we have a single word, there's just one ordering. We can code this up as follows:"]},{"cell_type":"code","metadata":{"id":"LSj3KM8Qr9b7","colab_type":"code","colab":{}},"source":["def factorial2(n):\n","     if n == 1:\n","         return 1\n","    else:\n","         return n * factorial2(n-1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A8crOtbxs6sP","colab_type":"text"},"source":["**Space-Time Tradeoffs**"]},{"cell_type":"markdown","metadata":{"id":"giH6NUUjs8BS","colab_type":"text"},"source":["We can sometimes significantly speed up the execution of a program by building an auxiliary data structure, such as an index. The following example implements a simple text retrieval system for the Movie Reviews Corpus. By indexing the document collection it provides much faster lookup."]},{"cell_type":"code","metadata":{"id":"laBLzt9vtJhI","colab_type":"code","colab":{}},"source":["def raw(file):\n","    contents = open(file).read()\n","    contents = re.sub(r'<.*?>', ' ', contents)\n","    contents = re.sub('\\s+', ' ', contents)\n","    return contents\n","\n","def snippet(doc, term):\n","    text = ' '*30 + raw(doc) + ' '*30\n","    pos = text.index(term)\n","    return text[pos-30:pos+30]\n","\n","print(\"Building Index...\")\n","files = nltk.corpus.movie_reviews.abspaths()\n","idx = nltk.Index((w, f) for f in files for w in raw(f).split())\n","\n","query = ''\n","while query != \"quit\":\n","    query = input(\"query> \")     # use raw_input() in Python 2\n","    if query in idx:\n","        for doc in idx[query]:\n","            print(snippet(doc, query))\n","    else:\n","        print(\"Not found\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ayVd0SVtn87","colab_type":"text"},"source":["**Vocabulary List**"]},{"cell_type":"markdown","metadata":{"id":"lbhVcyZCtrS7","colab_type":"text"},"source":["Maintaing a vocabulary list is an example of space-time tradeoff, if you need to process an input text to check that all words are in an existing vocabulary, the vocabulary should be stored as a set, not a list. The elements of a set are automatically indexed, so testing membership of a large set will be much faster than testing membership of the corresponding list.\n","\n"]},{"cell_type":"code","metadata":{"id":"1t3paasdt9i5","colab_type":"code","colab":{}},"source":["from timeit import Timer\n","vocab_size = 100000\n","setup_list = \"import random; vocab = range(%d)\" % vocab_size \n","#simulates a vocabulary of 100,000 items using a list\n","setup_set = \"import random; vocab = set(range(%d))\" % vocab_size\n","#or set of integers\n","statement = \"random.randint(0, %d) in vocab\" % (vocab_size * 2)    \n","#will generate a random item which has a 50% chance of being in the vocabulary\n","print(Timer(statement, setup_list).timeit(1000))\n","print(Timer(statement, setup_set).timeit(1000))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GNpwaGeEuwns","colab_type":"text"},"source":["**Result** Performing 1000 list membership tests takes a total of 2.8 seconds, while the equivalent tests on a set take a mere 0.0037 seconds, or three orders of magnitude faster."]},{"cell_type":"markdown","metadata":{"id":"7B9Q5KZPu4jr","colab_type":"text"},"source":["**Dynamic Programming**"]},{"cell_type":"markdown","metadata":{"id":"tBfVsOtfyr2n","colab_type":"text"},"source":["Dynamic programming is a particular method used in natural language processing to design algorithms. The word ' programming ' is used to mean planning or scheduling in a distinct context than you might expect. In the remainder of this section we will introduce dynamic programming, but in a rather different context to syntactic parsing."]},{"cell_type":"markdown","metadata":{"id":"xQGN5uo2y7zI","colab_type":"text"},"source":["For example, Pingala was an Indian author who lived around the 5th century B.C., and wrote a treatise on Sanskrit prosody called the Chandas Shastra. Virahanka extended this work around the 6th century A.D., studying the number of ways of combining short and long syllables to create a meter of length n. Short syllables, marked S, take up one unit of length, while long syllables, marked L, take two. Pingala found, for example, that there are five ways to construct a meter of length 4: V4 = {LL, SSL, SLS, LSS, SSSS}. Observe that we can split V4 into two subsets, those starting with L and those starting with S."]},{"cell_type":"markdown","metadata":{"id":"UgTLctAJzB4y","colab_type":"text"},"source":["V4 =\n","  LL, LSS\n","    i.e. L prefixed to each item of V2 = {L, SS}\n","  SSL, SLS, SSSS\n","    i.e. S prefixed to each item of V3 = {SL, LS, SSS}"]},{"cell_type":"code","metadata":{"id":"DU0SzfPUzGGa","colab_type":"code","colab":{}},"source":["def virahanka1(n):  #to compute meters\n","    if n == 0:\n","        return [\"\"]\n","    elif n == 1:\n","        return [\"S\"]\n","    else:\n","        s = [\"S\" + prosody for prosody in virahanka1(n-1)]\n","        l = [\"L\" + prosody for prosody in virahanka1(n-2)]\n","        return s + l\n","\n","def virahanka2(n):\n","    lookup = [[\"\"], [\"S\"]]\n","    for i in range(n-1):\n","        s = [\"S\" + prosody for prosody in lookup[i+1]]\n","        l = [\"L\" + prosody for prosody in lookup[i]]\n","        lookup.append(s + l)\n","    return lookup[n]\n","\n","def virahanka3(n, lookup={0:[\"\"], 1:[\"S\"]}):\n","    if n not in lookup:\n","        s = [\"S\" + prosody for prosody in virahanka3(n-1)]\n","        l = [\"L\" + prosody for prosody in virahanka3(n-2)]\n","        lookup[n] = s + l\n","    return lookup[n]\n","\n","from nltk import memoize  #stores the result of each previous call to the function along with the parameters that were used\n","@memoize\n","def virahanka4(n):\n","    if n == 0:\n","        return [\"\"]\n","    elif n == 1:\n","        return [\"S\"]\n","    else:\n","        s = [\"S\" + prosody for prosody in virahanka4(n-1)]\n","        l = [\"L\" + prosody for prosody in virahanka4(n-2)]\n","        return s + l"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tUkSWkvbzaen","colab_type":"text"},"source":["These are the Four Ways to Compute Sanskrit Meter: (i) recursive; (ii) bottom-up dynamic programming; (iii) top-down dynamic programming; and (iv) built-in memoization."]},{"cell_type":"markdown","metadata":{"id":"6NZbaTjHz4Xn","colab_type":"text"},"source":["![alt text](https://scontent.fist2-3.fna.fbcdn.net/v/t1.15752-9/64923758_696010734183994_3334067401170878464_n.png?_nc_cat=106&_nc_ht=scontent.fist2-3.fna&oh=98032284ef25bdf364474a38d9be57e0&oe=5D8F6044)"]},{"cell_type":"markdown","metadata":{"id":"hP0R0_fy2yEm","colab_type":"text"},"source":["**8. A Sample of Python Libraries**"]},{"cell_type":"markdown","metadata":{"id":"wTtVDYDL24C4","colab_type":"text"},"source":["**Matplotlib**"]},{"cell_type":"markdown","metadata":{"id":"4I0N7raRKxkp","colab_type":"text"},"source":["Python has some libraries that are useful for visualizing language data. The Matplotlib package supports sophisticated plotting functions with a MATLAB-style interface, and is available from http://matplotlib.sourceforge.net/.\n","\n","\n","So far we have focused on textual presentation and the use of formatted print statements to get output lined up in columns. It is often very useful to display numerical data in graphical form, since this often makes it easier to detect patterns."]},{"cell_type":"markdown","metadata":{"id":"VbtzDPlUK3Hi","colab_type":"text"},"source":["**Frequency of Modals in Different Sections of the Brown Corpus**"]},{"cell_type":"code","metadata":{"id":"tdmcgEStK71c","colab_type":"code","colab":{}},"source":["from numpy import arange\n","from matplotlib import pyplot\n","\n","colors = 'rgbcmyk' # red, green, blue, cyan, magenta, yellow, black\n","\n","def bar_chart(categories, words, counts):\n","    \"Plot a bar chart showing counts for each word by category\"\n","    ind = arange(len(words))\n","    width = 1 / (len(categories) + 1)\n","    bar_groups = []\n","    for c in range(len(categories)):\n","        bars = pyplot.bar(ind+c*width, counts[categories[c]], width,\n","                         color=colors[c % len(colors)])\n","        bar_groups.append(bars)\n","    pyplot.xticks(ind+width, words)\n","    pyplot.legend([b[0] for b in bar_groups], categories, loc='upper left')\n","    pyplot.ylabel('Frequency')\n","    pyplot.title('Frequency of Six Modal Verbs by Genre')\n","    pyplot.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IiZesFZzLbme","colab_type":"text"},"source":["![alt text](https://scontent.fist2-4.fna.fbcdn.net/v/t1.15752-9/65201016_615619188916293_7510837201479925760_n.png?_nc_cat=105&_nc_ht=scontent.fist2-4.fna&oh=5425428a9de35900134908b96799e0cc&oe=5DC39987)"]},{"cell_type":"markdown","metadata":{"id":"T-FeJQp_Ly7K","colab_type":"text"},"source":["ar Chart Showing Frequency of Modals in Different Sections of Brown Corpus: this visualization was produced by the program. From the bar chart it is immediately obvious that may and must have almost identical relative frequencies. The same goes for could and might."]},{"cell_type":"markdown","metadata":{"id":"FzN2s2jhL_qM","colab_type":"text"},"source":["**NetworkX** "]},{"cell_type":"markdown","metadata":{"id":"FH-UYK9lMicG","colab_type":"text"},"source":["The NetworkX package is for defining and manipulating structures consisting of nodes and edges, known as graphs. It is available from https://networkx.lanl.gov/. NetworkX can be used in conjunction with Matplotlib to visualize networks, such as WordNet (the semantic network)"]},{"cell_type":"markdown","metadata":{"id":"nZYadtB-MlUJ","colab_type":"text"},"source":["**Using the NetworkX and Matplotlib Libraries**"]},{"cell_type":"code","metadata":{"id":"OoHWrWRMMnwN","colab_type":"code","colab":{}},"source":["import networkx as nx\n","import matplotlib\n","from nltk.corpus import wordnet as wn\n","\n","def traverse(graph, start, node):\n","    graph.depth[node.name] = node.shortest_path_distance(start)\n","    for child in node.hyponyms():\n","        graph.add_edge(node.name, child.name) \n","        traverse(graph, start, child) \n","def hyponym_graph(start):\n","    G = nx.Graph() \n","    G.depth = {}\n","    traverse(G, start, start)\n","    return G\n","\n","def graph_draw(graph):\n","    nx.draw_graphviz(graph,\n","         node_size = [16 * graph.degree(n) for n in graph],\n","         node_color = [graph.depth[n] for n in graph],\n","         with_labels = False)\n","    matplotlib.pyplot.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZQtTtHd-QnAo","colab_type":"text"},"source":["![alt text](https://scontent.fist2-4.fna.fbcdn.net/v/t1.15752-9/65431496_2324504134482507_678848602226819072_n.png?_nc_cat=111&_nc_ht=scontent.fist2-4.fna&oh=c7e36c359596667a4af0ae77409b82e2&oe=5D7EE329)"]},{"cell_type":"markdown","metadata":{"id":"5wsP0kQEQ2Q2","colab_type":"text"},"source":["Visualization with NetworkX and Matplotlib: Part of the WordNet hypernym hierarchy is displayed, starting with dog.n.01 (the darkest node in the middle); node size is based on the number of children of the node, and color is based on the distance of the node from dog.n.01; this visualization was produced by the program.\n"]},{"cell_type":"markdown","metadata":{"id":"HUj0yBGgRBXU","colab_type":"text"},"source":["**csv**"]},{"cell_type":"markdown","metadata":{"id":"Ved-09l9RCkI","colab_type":"text"},"source":["Language analysis work often involves data tabulations, containing information about lexical items, or the participants in an empirical study, or the linguistic features extracted from a corpus. Here's a fragment of a simple lexicon, in CSV format:\n","\n","sleep, sli:p, v.i, a condition of body and mind ...\n","walk, wo:k, v.intr, progress by lifting and setting down each foot ...\n","wake, weik, intrans, cease to sleep"]},{"cell_type":"code","metadata":{"id":"PRBFw6k-RGda","colab_type":"code","colab":{}},"source":["import csv\n"," input_file = open(\"lexicon.csv\", \"rb\")  #to open a CSV file called lexicon.csv\n"," for row in csv.reader(input_file):   #iterate over rows\n","     print(row)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MfYvU9rVRgsd","colab_type":"text"},"source":["**NumPy**"]},{"cell_type":"markdown","metadata":{"id":"MZt2Xfq0Rh8V","colab_type":"text"},"source":["The NumPy package provides substantial support for numerical processing in Python. NumPy has a multi-dimensional array object, which is easy to initialize and access:"]},{"cell_type":"code","metadata":{"id":"oD-fysF7RrZp","colab_type":"code","colab":{}},"source":["from numpy import array\n"," cube = array([ [[0,0,0], [1,1,1], [2,2,2]],\n","                [[3,3,3], [4,4,4], [5,5,5]],\n","                [[6,6,6], [7,7,7], [8,8,8]] ])\n"," cube[1,1,1]\n"," cube[2].transpose()\n"," cube[2,1:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G8ZH5wysR9Kw","colab_type":"text"},"source":["NumPy includes linear algebra functions. Here we perform singular value decomposition on a matrix, an operation used in latent semantic analysis to help identify implicit concepts in a document collection."]},{"cell_type":"code","metadata":{"id":"C91zblA_R-ES","colab_type":"code","colab":{}},"source":["from numpy import linalg\n"," a=array([[4,0], [3,-5]])\n","u,s,vt = linalg.svd(a)\n","u\n","s\n","vt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mDNZ--c3ST6h","colab_type":"text"},"source":["**Other Python Libraries**"]},{"cell_type":"markdown","metadata":{"id":"Yj6RrMOcSV0W","colab_type":"text"},"source":["There are many other Python libraries, and you can search for them with the help of the Python Package Index http://pypi.python.org/. Many libraries provide an interface to external software, such as relational databases (e.g. mysql-python) and large document collections (e.g. PyLucene). Many other libraries give access to file formats such as PDF, MSWord, and XML (pypdf, pywin32, xml.etree), RSS feeds (e.g. feedparser), and electronic mail (e.g. imaplib, email)."]}]}