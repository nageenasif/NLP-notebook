{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"C8_NLPwithPython.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vCSAq3oBAmMI","colab_type":"text"},"source":["<h1><center>CHAPTER 8: Analyzing Sentence Structure</center></h1>"]},{"cell_type":"markdown","metadata":{"id":"4zQ4s6OjAw0a","colab_type":"text"},"source":[" We need a way to deal with the ambiguity that natural language is famous for. We also need to be able to cope with the fact that there are an unlimited number of possible sentences, and we can only write finite programs to analyze their structures and discover their meanings.\n","\n","The goal of this chapter is to answer the following questions:\n","\n","How can we use a formal grammar to describe the structure of an unlimited set of sentences?\n","How do we represent the structure of sentences using syntax trees?\n","How do parsers analyze a sentence and automatically build a syntax tree?"]},{"cell_type":"markdown","metadata":{"id":"aYXT7GCOFqbo","colab_type":"text"},"source":["**To be able to run the codes below, download nltk.**"]},{"cell_type":"code","metadata":{"id":"wob7TJB2FtEf","colab_type":"code","outputId":"b099f389-3f7f-4818-9544-7515815bc9ef","executionInfo":{"status":"ok","timestamp":1562921727054,"user_tz":-180,"elapsed":5970,"user":{"displayName":"Damla Nisa CEVIK","photoUrl":"","userId":"15647447958135404493"}},"colab":{"base_uri":"https://localhost:8080/","height":538}},"source":["import nltk\n","import random #shuffle\n","from nltk.corpus import names\n","from nltk.corpus import brown\n","from nltk.corpus import movie_reviews\n","from nltk.corpus import conll2000\n","\n","\n","nltk.download('punkt') #for word_tokenize\n","nltk.download('averaged_perceptron_tagger')#for pos tagger\n","nltk.download('tagsets') #for pos_tag help\n","nltk.download('universal_tagset') #universal tags for pos\n","\n","####Corpora###\n","nltk.download('names')\n","nltk.download('brown') #brown\n","nltk.download('nps_chat') #nps chat \n","nltk.download('conll2000') #conll \n","nltk.download('treebank') #penn \n","nltk.download('sinica_treebank') #sinica treebank\n","nltk.download('indian') #indian corpus\n","nltk.download('mac_morpho') #mac morpho\n","nltk.download('rte') # recognizing text entailment\n","nltk.download('senseval') #senseval\n","nltk.download('ppattach')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package tagsets to /root/nltk_data...\n","[nltk_data]   Unzipping help/tagsets.zip.\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n","[nltk_data] Downloading package names to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/names.zip.\n","[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n","[nltk_data] Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/nps_chat.zip.\n","[nltk_data] Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/conll2000.zip.\n","[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/treebank.zip.\n","[nltk_data] Downloading package sinica_treebank to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data] Downloading package indian to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/indian.zip.\n","[nltk_data] Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/mac_morpho.zip.\n","[nltk_data] Downloading package rte to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/rte.zip.\n","[nltk_data] Downloading package senseval to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/senseval.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"Iaw3u-szFtYS","colab_type":"text"},"source":["<h2>1. Some Grammatical Dilemmas</h2>"]},{"cell_type":"markdown","metadata":{"id":"eSH-w83EF0fs","colab_type":"text"},"source":["<h3>Linguistic Data and Unlimited Possibilities</h3>\n"]},{"cell_type":"markdown","metadata":{"id":"1_vEHe3mF3CV","colab_type":"text"},"source":[" Let's consider this data more closely, and make the thought experiment that we have a gigantic corpus consisting of everything that has been either uttered or written in English over, say, the last 50 years. Previous chapters have shown you how to process and analyse text corpora, and we have stressed the challenges for NLP in dealing with the vast amount of electronic language data that is growing daily. Would we be justified in calling this corpus \"the language of modern English\"? There are a number of reasons why we might answer No. \n"," \n"," \n"," Accordingly, we can argue that the \"modern English\" is not equivalent to the very big set of word sequences in our imaginary corpus. Speakers of English can make judgements about these sequences, and will reject some of them as being ungrammatical.\n","\n","Equally, it is easy to compose a new sentence and have speakers agree that it is perfectly good English. For example, sentences have an interesting property that they can be embedded inside larger sentences. Consider the following sentences:\n","\n","\t\n","a.\t\tUsain Bolt broke the 100m record\n","\n","b.\t\tThe Jamaica Observer reported that Usain Bolt broke the 100m record\n","\n","c.\t\tAndre said The Jamaica Observer reported that Usain Bolt broke the 100m record\n","\n","d.\t\tI think Andre said the Jamaica Observer reported that Usain Bolt broke the 100m record\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CbLY5Gj4TuDH","colab_type":"text"},"source":["If we replaced whole sentences with the symbol S, we would see patterns like Andre said S and I think S. These are templates for taking a sentence and constructing a bigger sentence. There are other templates we can use, like S but S, and S when S. With a bit of ingenuity we can construct some really long sentences using these templates. "]},{"cell_type":"markdown","metadata":{"id":"PYqPVNiCT2Ga","colab_type":"text"},"source":["**Here's an impressive example from a Winnie the Pooh story**"]},{"cell_type":"markdown","metadata":{"id":"vdvg0JdcT6qZ","colab_type":"text"},"source":["[You can imagine Piglet's joy when at last the ship came in sight of him.] In after-years he liked to think that he had been in Very Great Danger during the Terrible Flood, but the only danger he had really been in was the last half-hour of his imprisonment, when Owl, who had just flown up, sat on a branch of his tree to comfort him, and told him a very long story about an aunt who had once laid a seagull's egg by mistake, and the story went on and on, rather like this sentence, until Piglet who was listening out of his window without much hope, went to sleep quietly and naturally, slipping slowly out of the window towards the water until he was only hanging on by his toes, at which moment, luckily, a sudden loud squawk from Owl, which was really part of the story, being what his aunt said, woke the Piglet up and just gave him time to jerk himself back into safety and say, \"How interesting, and did she?\" when — well, you can imagine his joy when at last he saw the good ship, Brain of Pooh (Captain, C. Robin; 1st Mate, P. Bear) coming over the sea to rescue him...\n"]},{"cell_type":"markdown","metadata":{"id":"MGO4V03HT_M3","colab_type":"text"},"source":["This long sentence actually has a simple structure that begins S but S when S. We can see from this example that language provides us with constructions which seem to allow us to extend sentences indefinitely. It is also striking that we can understand sentences of arbitrary length that we've never heard before: it's not hard to concoct an entirely novel sentence, one that has probably never been used before in the history of the language, yet all speakers of the language will understand it.\n","\n","The purpose of a grammar is to give an explicit description of a language. But the way in which we think of a grammar is closely intertwined with what we consider to be a language. Is it a large but finite set of observed utterances and written texts? Is it something more abstract like the implicit knowledge that competent speakers have about grammatical sentences? Or is it some combination of the two? We won't take a stand on this issue, but instead will introduce the main approaches.\n","\n","In this chapter, we will adopt the formal framework of \"generative grammar\", in which a \"language\" is considered to be nothing more than an enormous collection of all grammatical sentences, and a grammar is a formal notation that can be used for \"generating\" the members of this set. Grammars use recursive productions of the form S → S and S."]},{"cell_type":"markdown","metadata":{"id":"89Ef1ukKIlTF","colab_type":"text"},"source":["**Ubiquitous Ambiguity**"]},{"cell_type":"markdown","metadata":{"id":"wO5gSBR9UVMt","colab_type":"text"},"source":["A well-known example of ambiguity is shown below from the Groucho Marx movie, Animal Crackers (1930):\n","\n","For example, While hunting in Africa, I shot an elephant in my pajamas. How an elephant got into my pajamas I'll never know.\n","\n","Let's take a closer look at the ambiguity in the phrase: I shot an elephant in my pajamas. First we need to define a simple grammar:"]},{"cell_type":"code","metadata":{"id":"37j5YEXsUjag","colab_type":"code","colab":{}},"source":["from nltk import CFG\n","groucho_grammar = CFG.fromstring(\"\"\"\n","S -> NP VP\n","PP -> P NP\n","NP -> Det N | Det N PP | 'I'\n","VP -> V NP | VP PP\n","Det -> 'an' | 'my'\n","N -> 'elephant' | 'pajamas'\n","V -> 'shot'\n","P -> 'in'\n","\"\"\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rULxJoF4U23a","colab_type":"text"},"source":["This grammar permits the sentence to be analyzed in two ways, depending on whether the prepositional phrase in my pajamas describes the elephant or the shooting event."]},{"cell_type":"code","metadata":{"id":"o3OE2UlvU3o0","colab_type":"code","outputId":"c077370b-bf70-469b-e0dd-2682b9370267","executionInfo":{"status":"ok","timestamp":1562922189322,"user_tz":-180,"elapsed":644,"user":{"displayName":"Damla Nisa CEVIK","photoUrl":"","userId":"15647447958135404493"}},"colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n","parser = nltk.ChartParser(groucho_grammar)\n","trees = parser.parse(sent)\n","for tree in trees:\n","  print (tree)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(S\n","  (NP I)\n","  (VP\n","    (VP (V shot) (NP (Det an) (N elephant)))\n","    (PP (P in) (NP (Det my) (N pajamas)))))\n","(S\n","  (NP I)\n","  (VP\n","    (V shot)\n","    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gFQUiEavVIco","colab_type":"text"},"source":["The program produces two bracketed structures, which we can depict as trees:"]},{"cell_type":"markdown","metadata":{"id":"fr87tu2BVJrd","colab_type":"text"},"source":["![alt text](https://scontent.fist1-1.fna.fbcdn.net/v/t1.15752-9/66532603_2280423715619995_8833154114616557568_n.png?_nc_cat=105&_nc_oc=AQkXkKpKVPGzvLA5BhYSU9iPLwPQBJmLCU3dkPzQ1iA6kO4qBSaI6s1nrbNSuYCtCD8&_nc_ht=scontent.fist1-1.fna&oh=ee12a873c0c5a2a8f771353b400c7956&oe=5DB4572F)"]},{"cell_type":"markdown","metadata":{"id":"sl4KkEciWBmI","colab_type":"text"},"source":["Notice that there's no ambiguity concerning the meaning of any of the words; e.g. the word shot doesn't refer to the act of using a gun in the first sentence, and using a camera in the second sentence."]},{"cell_type":"markdown","metadata":{"id":"yYmo-T6qWFTc","colab_type":"text"},"source":["**Your Turn** Consider the following sentences and see if you can think of two quite different interpretations: Fighting animals could be dangerous. Visiting relatives can be tiresome. Is ambiguity of the individual words to blame? If not, what is the cause of the ambiguity?"]},{"cell_type":"markdown","metadata":{"id":"3NdamMDjWMfX","colab_type":"text"},"source":["## 2. What's the Use of Syntax?\n","\n","**Beyond n-grams**"]},{"cell_type":"markdown","metadata":{"id":"-fX7Tx-iWWlO","colab_type":"text"},"source":["We saw an example above of how to use the frequency information in bigrams to generate text that seems perfectly acceptable for small sequences of words but rapidly degenerates into nonsense. Here's another pair of examples that we created by computing the bigrams over the text of a childrens' story, The Adventures of Buster Brown (http://www.gutenberg.org/files/22816/22816.txt):\n","\n","(4)\t\t\n","a.\t\tHe roared with me the pail slip down his back\n","\n","b.\t\tThe worst part and clumsy looking for whoever heard light\n","\n","\n","You intuitively know that these sequences are \"word-salad\", but you probably find it hard to pin down what's wrong with them. One benefit of studying grammar is that it provides a conceptual framework and vocabulary for spelling out these intuitions. Let's take a closer look at the sequence the worst part and clumsy looking. This looks like a coordinate structure, where two phrases are joined by a coordinating conjunction such as and, but or or. Here's an informal (and simplified) statement of how coordination works syntactically:\n","\n","Coordinate Structure:\n","\n","If v1 and v2 are both phrases of grammatical category X, then v1 and v2 is also a phrase of category X.\n","Here are a couple of examples. In the first, two NPs (noun phrases) have been conjoined to make an NP, while in the second, two APs (adjective phrases) have been conjoined to make an AP.\n","\n","(5)\t\t\n","a.\t\tThe book's ending was (NP the worst part and the best part) for me.\n","\n","b.\t\tOn land they are (AP slow and clumsy looking).\n","\n","\n","What we can't do is conjoin an NP and an AP, which is why the worst part and clumsy looking is ungrammatical. Before we can formalize these ideas, we need to understand the concept of constituent structure.\n","\n","Constituent structure is based on the observation that words combine with other words to form units. The evidence that a sequence of words forms such a unit is given by substitutability — that is, a sequence of words in a well-formed sentence can be replaced by a shorter sequence without rendering the sentence ill-formed. To clarify this idea, consider the following sentence:\n","\n","(6)\t\tThe little bear saw the fine fat trout in the brook.\n","\n","The fact that we can substitute He for The little bear indicates that the latter sequence is a unit. By contrast, we cannot replace little bear saw in the same way.\n","\n","(7)\t\t\n","a.\t\tHe saw the fine fat trout in the brook.\n","\n","b.\t\t*The he the fine fat trout in the brook.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6I64f8ZhWj5t","colab_type":"text"},"source":["**Substitution of Word Sequences:** Working from the top row, we can replace particular sequences of words (e.g. the brook) with individual words (e.g. it); repeating this process we arrive at a grammatical two-word sentence.\n"]},{"cell_type":"markdown","metadata":{"id":"0u68JCClW6IX","colab_type":"text"},"source":["![alt text](https://scontent.fist1-2.fna.fbcdn.net/v/t1.15752-9/66391169_326583831564252_3603630423525031936_n.png?_nc_cat=101&_nc_oc=AQnmf4uX--BzZUq_bBP86ABplN9k5R0eORTTEZhJq_5lelDnT0OVRFdF3by9Jn6R0Cg&_nc_ht=scontent.fist1-2.fna&oh=b80cb83a9e62e6a98370a19e7d1bdbd1&oe=5DBCE162)"]},{"cell_type":"markdown","metadata":{"id":"YFi672g6W_nS","colab_type":"text"},"source":["**Substitution of Word Sequences Plus Grammatical Categories:** Here, we have added grammatical category labels to the words we saw in the earlier figure. The labels NP, VP, and PP stand for noun phrase, verb phrase and prepositional phrase respectively."]},{"cell_type":"markdown","metadata":{"id":"zv6Cv0LiXLOO","colab_type":"text"},"source":["![alt text](https://scontent.fist1-2.fna.fbcdn.net/v/t1.15752-9/66668976_441666776428715_171090851164848128_n.png?_nc_cat=109&_nc_oc=AQnOLonWuqda8HcniLXBBlw0ZfFKbaJRXdxjffT7fe-DTmYC3Xrxrd3IxdFSE_pmrjc&_nc_ht=scontent.fist1-2.fna&oh=8ce8bee81c3403ebcd0a5c3bce43ba35&oe=5DA52EF5)"]},{"cell_type":"markdown","metadata":{"id":"SxZGLR51XWtY","colab_type":"text"},"source":["Now if we strip out the words apart from the topmost row, add an S node, and flip the figure over, we end up with a standard phrase structure tree, Each node in this tree (including the words) is called a constituent. The immediate constituents of S are NP and VP.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iT90Ys73XfOE","colab_type":"text"},"source":["![alt text](https://scontent.fist1-1.fna.fbcdn.net/v/t1.15752-9/66314181_1012062082332697_210592670408507392_n.png?_nc_cat=102&_nc_oc=AQnW-aq2Kc_sbVUCd1-Rf-DNR0pMynGVvqd8Vth9ruiYmmGgceT_nLzwzTxzahMC9Xo&_nc_ht=scontent.fist1-1.fna&oh=4bbd78a33d78a0feb3519fea0b121dd8&oe=5DBA5255)"]},{"cell_type":"markdown","metadata":{"id":"6KNP5GqEF-wU","colab_type":"text"},"source":["##3. Context Free Grammar\n","\n","###A Simple Grammar\n","\n","Let's start off by looking at a simple context-free grammar. By convention, the left-hand-side of the first production is the **start-symbol** of the grammar, typically `S`, and all well-formed trees must have this symbol as their root label. Now we define a grammar and show how to parse a simple sentence admitted by the grammar.\n","\n","In NLTK, context-free grammars are defined in the nltk.grammar module."]},{"cell_type":"code","metadata":{"id":"a6CnzhDyG0mE","colab_type":"code","outputId":"49a2775f-c3ee-449f-fa23-305f01c214d5","executionInfo":{"status":"ok","timestamp":1562762403566,"user_tz":-180,"elapsed":1146,"user":{"displayName":"Atalay Küçükoğlu","photoUrl":"","userId":"03845945074394485298"}},"colab":{"base_uri":"https://localhost:8080/","height":454}},"source":["grammar1 = nltk.CFG.fromstring(\"\"\"\n","  S -> NP VP \n","  VP -> V NP | V NP PP\n","  PP -> P NP\n","  V -> \"saw\" | \"ate\" | \"walked\"\n","  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n","  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n","  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n","  P -> \"in\" | \"on\" | \"by\" | \"with\"\n","  \"\"\")\n","\n","\n"," \t\n","sent = \"Mary saw Bob\".split()\n","rd_parser = nltk.RecursiveDescentParser(grammar1)\n","for tree in rd_parser.parse(sent):\n","     print(tree)\n","        \n","for p in grammar1.productions():\n","    print(p)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(S (NP Mary) (VP (V saw) (NP Bob)))\n","S -> NP VP\n","VP -> V NP\n","VP -> V NP PP\n","PP -> P NP\n","V -> 'saw'\n","V -> 'ate'\n","V -> 'walked'\n","NP -> 'John'\n","NP -> 'Mary'\n","NP -> 'Bob'\n","NP -> Det N\n","NP -> Det N PP\n","Det -> 'a'\n","Det -> 'an'\n","Det -> 'the'\n","Det -> 'my'\n","N -> 'man'\n","N -> 'dog'\n","N -> 'cat'\n","N -> 'telescope'\n","N -> 'park'\n","P -> 'in'\n","P -> 'on'\n","P -> 'by'\n","P -> 'with'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tWtRrdtwG_YE","colab_type":"text"},"source":["Here,  ```S -> NP VP``` means a sentence consists of a noun phrase NP and a verb phrase VP. Similarly, ```VP -> V NP | V NP PP``` means a verb phrase `VP` consist of either a verb `V` and a noun phrase `NP`, or a verb `V`, noun phrase `NP` and a prepositional phrase `PP`.\n","\n","\n","`V -> \"saw\" | \"ate\" | \"walked\"` represents the vocabulary for the verbs."]},{"cell_type":"markdown","metadata":{"id":"T98dMMnXJj0_","colab_type":"text"},"source":["**Recursive Descent Parser Demo:** This tool allows you to watch the operation of a recursive descent parser as it grows the parse tree and matches it against the input words.\n","\n","![](https://www.nltk.org/images/parse_rdparsewindow.png)"]},{"cell_type":"markdown","metadata":{"id":"PQH1s-sUJtBT","colab_type":"text"},"source":["If we parse the sentence The dog saw a man in the park using the grammar shown in 3.1, we end up with two trees:\n","\n","a) \n","\n","![](https://www.nltk.org/book/tree_images/ch08-tree-4.png)\n","\n","b) \n","\n","![alt text](https://www.nltk.org/book/tree_images/ch08-tree-5.png)\n","\n","Since our grammar licenses two trees for this sentence, the sentence is said to be **structurally ambiguous**. The ambiguity in question is called a prepositional phrase attachment ambiguity. When the `PP` is attached to `VP`, the intended interpretation is that the seeing event happened in the park. However, if the `PP` is attached to `NP`, then it was the man who was in the park, and the agent of the seeing (the dog) might have been sitting on the balcony of an apartment overlooking the park."]},{"cell_type":"markdown","metadata":{"id":"TtjUnu00LWwI","colab_type":"text"},"source":["###Writing Your Own Grammars\n","\n","If you are interested in experimenting with writing CFGs, you will find it helpful to create and edit your grammar in a text file, say mygrammar.cfg. You can then load it into NLTK as follows:\n","\n","\n","\n","```\n","grammar1 = nltk.data.load('file:mygrammar.cfg')\n","\n","\n","```\n","\n","\n","If the command print(tree) produces no output, this is probably because your sentence  sent is not admitted by your grammar. In this case, call the parser with tracing set to be on: "]},{"cell_type":"code","metadata":{"id":"Vw3Ux1U8iCWZ","colab_type":"code","colab":{}},"source":["rd_parser = nltk.RecursiveDescentParser(grammar1, trace=2)\n","for tree in rd_parser.parse(sent):\n","    print(tree)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-K0_dIGMigge","colab_type":"text"},"source":["You can also check what productions are currently in the grammar with the command:"]},{"cell_type":"code","metadata":{"id":"sOGNz-_Iii7M","colab_type":"code","outputId":"395195f7-4bb3-4a0a-8621-a3524240e88b","executionInfo":{"status":"ok","timestamp":1562762403575,"user_tz":-180,"elapsed":1138,"user":{"displayName":"Atalay Küçükoğlu","photoUrl":"","userId":"03845945074394485298"}},"colab":{"base_uri":"https://localhost:8080/","height":437}},"source":["for p in grammar1.productions(): \n","    print(p)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["S -> NP VP\n","VP -> V NP\n","VP -> V NP PP\n","PP -> P NP\n","V -> 'saw'\n","V -> 'ate'\n","V -> 'walked'\n","NP -> 'John'\n","NP -> 'Mary'\n","NP -> 'Bob'\n","NP -> Det N\n","NP -> Det N PP\n","Det -> 'a'\n","Det -> 'an'\n","Det -> 'the'\n","Det -> 'my'\n","N -> 'man'\n","N -> 'dog'\n","N -> 'cat'\n","N -> 'telescope'\n","N -> 'park'\n","P -> 'in'\n","P -> 'on'\n","P -> 'by'\n","P -> 'with'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eV7QC5glipEf","colab_type":"text"},"source":["###Recursion in Syntactic Structure\n","\n","A grammar is said to be recursive if a category occurring on the left hand side of a production also appears on the righthand side of a production."]},{"cell_type":"code","metadata":{"id":"akp0CNoTj8KK","colab_type":"code","colab":{}},"source":["grammar2 = nltk.CFG.fromstring(\"\"\"\n","  S  -> NP VP\n","  NP -> Det Nom | PropN\n","  Nom -> Adj Nom | N\n","  VP -> V Adj | V NP | V S | V NP PP\n","  PP -> P NP\n","  PropN -> 'Buster' | 'Chatterer' | 'Joe'\n","  Det -> 'the' | 'a'\n","  N -> 'bear' | 'squirrel' | 'tree' | 'fish' | 'log'\n","  Adj  -> 'angry' | 'frightened' |  'little' | 'tall'\n","  V ->  'chased'  | 'saw' | 'said' | 'thought' | 'was' | 'put'\n","  P -> 'on'\n","  \"\"\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FAXxFrUwj9C1","colab_type":"text"},"source":["To see how recursion arises from this grammar, consider the following trees. (a) involves nested nominal phrases, while (b) contains nested sentences.\n","\n","a)\n","\n","![](https://www.nltk.org/book/tree_images/ch08-tree-6.png)\n","\n","b) \n","\n","![](https://www.nltk.org/book/tree_images/ch08-tree-7.png)\n","\n","Further explanation about recursive parsing is given in the next section."]},{"cell_type":"markdown","metadata":{"id":"X5bOmocXke_S","colab_type":"text"},"source":["##4. Parsing With Context Free Grammar\n","\n","A **parser** processes input sentences according to the productions of a grammar, and builds one or more constituent structures that conform to the grammar. A grammar is a declarative specification of well-formedness — it is actually just a string, not a program. A parser is a procedural interpretation of the grammar. \n","\n","Many natural language applications involve parsing at some point; for example, we would expect the natural language questions submitted to a question-answering system to undergo parsing as an initial step."]},{"cell_type":"markdown","metadata":{"id":"y3yldWpi1DoC","colab_type":"text"},"source":["###Recursive Descent Parsing\n","\n","The simplest kind of parser interprets a grammar as a specification of how to break a high-level goal into several lower-level subgoals. The top-level goal is to find an S. The `S → NP VP` production permits the parser to replace this goal with two subgoals: find an `NP`, then find a `VP`. Each of these subgoals can be replaced in turn by sub-sub-goals, using productions that have `NP` and `VP` on their left-hand side. Eventually, this expansion process leads to subgoals such as: find the word *telescope*. Such subgoals can be directly compared against the input sequence, and succeed if the next word is matched. If there is no match the parser must back up and try a different alternative.\n","\n","![](https://www.nltk.org/images/rdparser1-6.png)"]},{"cell_type":"markdown","metadata":{"id":"8GMltmdd2act","colab_type":"text"},"source":["Six Stages of a Recursive Descent Parser: the parser begins with a tree consisting of the node S; at each stage it consults the grammar to find a production that can be used to enlarge the tree; when a lexical production is encountered, its word is compared against the input; after a complete parse has been found, the parser backtracks to look for more parses.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"c1bHjyJV30L3","colab_type":"text"},"source":["**Note:**\n","\n","RecursiveDescentParser() takes an optional parameter trace. If trace is greater than zero, then the parser will report the steps that it takes as it parses a text."]},{"cell_type":"markdown","metadata":{"id":"wTOvEEwt3w_4","colab_type":"text"},"source":["**Shortcomings of Recursive Descent Parsing:**\n","\n","\n","\n","1.   Left-recursive productions like NP -> NP PP send it into an infinite loop.\n","2.   The parser wastes a lot of time considering words and structures that do not correspond to the input sentence.\n","3.   Third, the backtracking process may discard parsed constituents that will need to be rebuilt again later. For example, backtracking over VP -> V NP will discard the subtree created for the NP. If the parser then proceeds with VP -> V NP PP, then the NP subtree must be created all over again.\n"]},{"cell_type":"markdown","metadata":{"id":"NyGy7jmt3q-x","colab_type":"text"},"source":["###Shift-Reduce Parsing\n","\n","A simple kind of bottom-up parser is the **shift-reduce parser**. In common with all bottom-up parsers, a shift-reduce parser tries to find sequences of words and phrases that correspond to the *right hand* side of a grammar production, and replace them with the left-hand side, until the whole sentence is reduced to an `S`.\n","\n","The shift-reduce parser repeatedly pushes the next input word onto a stack (4.1); this is the **shift** operation. If the top n items on the stack match the n items on the right hand side of some production, then they are all popped off the stack, and the item on the left-hand side of the production is pushed on the stack. This replacement of the top n items with a single item is the **reduce** operation. The parser finishes when all the input is consumed and there is only one item remaining on the stack, a parse tree with an S node as its root. Six stages of the execution of this parser are shown:"]},{"cell_type":"markdown","metadata":{"id":"b5Qm_G-n5QTc","colab_type":"text"},"source":["![](https://www.nltk.org/images/srparser1-6.png)"]},{"cell_type":"code","metadata":{"id":"l-uYuFDE5ZCU","colab_type":"code","outputId":"8b375a5c-cc77-4da1-d94b-e8f157146bcc","executionInfo":{"status":"ok","timestamp":1562762424476,"user_tz":-180,"elapsed":717,"user":{"displayName":"Atalay Küçükoğlu","photoUrl":"","userId":"03845945074394485298"}},"colab":{"base_uri":"https://localhost:8080/","height":252}},"source":["sr_parser = nltk.ShiftReduceParser(grammar1, trace=2)\n","sent = 'Mary saw a dog'.split()\n","for tree in sr_parser.parse(sent):\n","    print(tree)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Parsing 'Mary saw a dog'\n","    [ * Mary saw a dog]\n","  S [ 'Mary' * saw a dog]\n","  R [ NP * saw a dog]\n","  S [ NP 'saw' * a dog]\n","  R [ NP V * a dog]\n","  S [ NP V 'a' * dog]\n","  R [ NP V Det * dog]\n","  S [ NP V Det 'dog' * ]\n","  R [ NP V Det N * ]\n","  R [ NP V NP * ]\n","  R [ NP VP * ]\n","  R [ S * ]\n","(S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tScNDHwg6Xx-","colab_type":"text"},"source":["A shift-reduce parser can reach a dead end and fail to find any parse, even if the input sentence is well-formed according to the grammar. When this happens, no input remains, and the stack contains items which cannot be reduced to an S. The problem arises because there are choices made earlier that cannot be undone by the parser (although users of the graphical demonstration can undo their choices). There are two kinds of choices to be made by the parser: (a) which reduction to do when more than one is possible (b) whether to shift or reduce when either action is possible.\n","\n","The advantage of shift-reduce parsers over recursive descent parsers is that they only build structure that corresponds to the words in the input. Furthermore, they only build each sub-structure once, e.g.  NP(Det(the), N(man)) is only built and pushed onto the stack a single time, regardless of whether it will later be used by the VP -> V NP PP reduction or the NP -> NP PP reduction."]},{"cell_type":"markdown","metadata":{"id":"_LaCPJoG6zmH","colab_type":"text"},"source":["###The Left-Corner Parser\n","\n","One of the problems with the recursive descent parser is that it goes into an infinite loop when it encounters a left-recursive production. This is because it applies the grammar productions blindly, without considering the actual input sentence. A left-corner parser is a hybrid between the bottom-up and top-down approaches we have seen.\n","\n","The key idea of left-corner parsing is to combine top-down processing with bottom-up processing in order to avoid going wrong in the ways that we are prone to go wrong with pure top-down and pure bottom-up techniques. Before we look at how this is done, you have to know what is the left corner of a rule. The left corner of a rule is the first symbol on the right hand side. For example, NP is the left corner of the rule `S -> NP VP`, and the is the left corner of the rule `Det -> 'the'`.\n","\n","A left-corner parser starts with a top-down prediction fixing the category that is to be recognized, like for example `S`. Next, it takes a bottom-up step and then alternates bottom-up and top-down steps until it has reached an `S`.\n","\n","Grammar `grammar1` allows us to produce the following parse of *John saw Mary*:\n","\n","![](https://www.nltk.org/book/tree_images/ch08-tree-8.png)\n","\n","Recall that the grammar (defined in 3) has the following productions for expanding NP:\n","\n","a.\t\tNP -> Det N\n","\n","b.\t\tNP -> Det N PP\n","\n","c.\t\tNP -> \"John\" | \"Mary\" | \"Bob\""]},{"cell_type":"markdown","metadata":{"id":"F1CuKOlGAFws","colab_type":"text"},"source":["If you were asked to select one of the rules for `NP` for this particular sentence, you would select (c). Once left corner parser finds out that 'John' is in the top category `NP`, it starts searching from bottom categories like (c), not from (a) or (b) because they contain also sub-categories. When it finds \"John\" as a left corner of `NP` it starts searching another rule that 'NP' is the left corner and so on.\n","\n","When left corner parser reaches the top rule possible, like `S -> NP VP`, by finishing `NP`sub-tree, it starts from the other element of the rule, `VP` and does similar things to construct the tree.\n","\n","A further and more visualized explanation can be seen in the following website: http://cs.union.edu/~striegnk/courses/nlp-with-prolog/html/node53.html"]},{"cell_type":"markdown","metadata":{"id":"hENradncDCny","colab_type":"text"},"source":["###Well-Formed Substring Tables\n","\n","The simple parsers discussed above suffer from limitations in both completeness and efficiency. In order to remedy these, we will apply the algorithm design technique of dynamic programming to the parsing problem. This approach to parsing is known as chart parsing. We introduce the main idea in this section; see the online materials available for this chapter for more implementation details.\n","\n","Dynamic programming allows us to build the `PP` *in my pajamas* just once. The first time we build it we save it in a table, then we look it up when we need to use it as a subconstituent of either the object NP or the higher VP. This table is known as a **well-formed substring table**, or WFST for short. (The term \"substring\" refers to a contiguous sequence of words within a sentence.)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ytUSJLvdIDn5","colab_type":"text"},"source":["***\n","\n","**Here is a good, visulized and easy to understand explanation of filling the WFST chart with CYK algorithm, we highly recommend you to check this material before continuing:**\n","\n","http://www.sfs.uni-tuebingen.de/~dm/04/winter/684.01/slides/09-single.pdf\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"eJPeRXoZI_9x","colab_type":"text"},"source":["Let's take the sentence *I shot an elephant in my pajamas.* as an example. The indexing of this sentence will be:\n","\n","![alt text](https://www.nltk.org/images/chart_positions1.png)\n"]},{"cell_type":"markdown","metadata":{"id":"qbFVbG0iG0xW","colab_type":"text"},"source":["For our WFST, we create an *(n-1) × (n-1)* matrix as a list of lists in Python, and initialize it with the lexical categories of each token, in the `init_wfst()` function. We also define a utility function `display()` to pretty-print the WFST for us."]},{"cell_type":"code","metadata":{"id":"g1LOJAjfGtf3","colab_type":"code","outputId":"4f3a2ea2-63ee-4949-c6c5-3e33bd4fc699","executionInfo":{"status":"ok","timestamp":1562850587063,"user_tz":-180,"elapsed":848,"user":{"displayName":"Atalay Küçükoğlu","photoUrl":"","userId":"03845945074394485298"}},"colab":{"base_uri":"https://localhost:8080/","height":370}},"source":["groucho_grammar = nltk.CFG.fromstring(\"\"\"\n","S -> NP VP\n","PP -> P NP\n","NP -> Det N | Det N PP | 'I'\n","VP -> V NP | VP PP\n","Det -> 'an' | 'my'\n","N -> 'elephant' | 'pajamas'\n","V -> 'shot'\n","P -> 'in'\n","\"\"\")\n","\n","def init_wfst(tokens, grammar):\n","    numtokens = len(tokens)\n","    wfst = [[None for i in range(numtokens+1)] for j in range(numtokens+1)]\n","    for i in range(numtokens):\n","        productions = grammar.productions(rhs=tokens[i])\n","        wfst[i][i+1] = productions[0].lhs()\n","    return wfst\n","\n","def complete_wfst(wfst, tokens, grammar, trace=False):\n","    index = dict((p.rhs(), p.lhs()) for p in grammar.productions())\n","    numtokens = len(tokens)\n","    for span in range(2, numtokens+1):\n","        for start in range(numtokens+1-span):\n","            end = start + span\n","            for mid in range(start+1, end):\n","                nt1, nt2 = wfst[start][mid], wfst[mid][end]\n","                if nt1 and nt2 and (nt1,nt2) in index:\n","                    wfst[start][end] = index[(nt1,nt2)]\n","                    if trace:\n","                        print(\"[%s] %3s [%s] %3s [%s] ==> [%s] %3s [%s]\" % \\\n","                        (start, nt1, mid, nt2, end, start, index[(nt1,nt2)], end))\n","    return wfst\n","\n","def display(wfst, tokens):\n","    print('\\nWFST ' + ' '.join((\"%-4d\" % i) for i in range(1, len(wfst))))\n","    for i in range(len(wfst)-1):\n","        print(\"%d   \" % i, end=\" \")\n","        for j in range(1, len(wfst)):\n","            print(\"%-4s\" % (wfst[i][j] or '.'), end=\" \")\n","        print()\n","        \n","print(\"Initialization:\")\n","tokens = \"I shot an elephant in my pajamas\".split()\n","wfst0 = init_wfst(tokens, groucho_grammar)\n","display(wfst0, tokens)\n","print(\"\\nCompleted chart:\")\n","wfst1 = complete_wfst(wfst0, tokens, groucho_grammar)\n","display(wfst1, tokens)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialization:\n","\n","WFST 1    2    3    4    5    6    7   \n","0    NP   .    .    .    .    .    .    \n","1    .    V    .    .    .    .    .    \n","2    .    .    Det  .    .    .    .    \n","3    .    .    .    N    .    .    .    \n","4    .    .    .    .    P    .    .    \n","5    .    .    .    .    .    Det  .    \n","6    .    .    .    .    .    .    N    \n","\n","Completed chart:\n","\n","WFST 1    2    3    4    5    6    7   \n","0    NP   .    .    S    .    .    S    \n","1    .    V    .    VP   .    .    VP   \n","2    .    .    Det  NP   .    .    .    \n","3    .    .    .    N    .    .    .    \n","4    .    .    .    .    P    .    PP   \n","5    .    .    .    .    .    Det  NP   \n","6    .    .    .    .    .    .    N    \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wLo8jEnpJQZD","colab_type":"text"},"source":["After completing chart, the relations between the words in the sentence according to the grammar given will be:\n","\n","![alt text](https://www.nltk.org/images/chart_positions2.png)\n","\n","Notice that we have not used any built-in parsing functions here. We've implemented a complete, primitive chart parser from the ground up!\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FMLJVQqGeiCP","colab_type":"text"},"source":["<h2>5. Dependencies and Dependency Grammar</h2>"]},{"cell_type":"markdown","metadata":{"id":"Rn-nrVfaeh_a","colab_type":"text"},"source":["Phrase structure grammar is concerned with how words and sequences of words combine to form constituents. A distinct and complementary approach, dependency grammar, focusses instead on how words relate to other words. Dependency is a binary asymmetric relation that holds between a **head** and its **dependents**. The head of a sentence is usually taken to be the tensed verb, and every other word is either dependent on the sentence head, or connects to it through a path of dependencies.\n","\n","A dependency representation is a labeled directed graph, where the nodes are the lexical items and the labeled arcs represent dependency relations from heads to dependents.\n","\n","Example for dependency graph, where arrows point from heads to their dependents:\n","![alt text](https://i.hizliresim.com/VQvdkq.png)\n","\n","The arcs in the above example are labeled with the grammatical function that holds between a dependent and its head. For example, *I* is the `SBJ` (subject) of *shot* (which is the head of the whole sentence), and in is an `NMOD` (noun modifier of *elephant*). In contrast to phrase structure grammar, therefore, dependency grammars can be used to directly express grammatical functions as a type of dependency.\n","\n","Here's one way of encoding a dependency grammar in NLTK — note that it only captures bare dependency information without specifying the type of dependency:"]},{"cell_type":"code","metadata":{"id":"38Ethl_5uo72","colab_type":"code","outputId":"6c86d036-6387-4b2c-bdb6-19e6fe509f40","executionInfo":{"status":"ok","timestamp":1562916966546,"user_tz":-180,"elapsed":896,"user":{"displayName":"Damla Nisa CEVIK","photoUrl":"","userId":"15647447958135404493"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["groucho_dep_grammar = nltk.DependencyGrammar.fromstring(\"\"\"\n","  'shot' -> 'I' | 'elephant' | 'in'\n","  'elephant' -> 'an' | 'in'\n","  'in' -> 'pajamas'\n","  'pajamas' -> 'my'\n","  \"\"\")\n","print(groucho_dep_grammar)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Dependency grammar with 7 productions\n","  'shot' -> 'I'\n","  'shot' -> 'elephant'\n","  'shot' -> 'in'\n","  'elephant' -> 'an'\n","  'elephant' -> 'in'\n","  'in' -> 'pajamas'\n","  'pajamas' -> 'my'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"m95C3Mwneh9D","colab_type":"text"},"source":["A dependency graph is **projective** if, when all the words are written in linear order, the edges can be drawn above the words without crossing. This is equivalent to saying that a word and all its descendents (dependents and dependents of its dependents, etc.) form a contiguous sequence of words within the sentence.\n","\n","The next example shows how `groucho_dep_grammar` provides an alternative approach to capturing the attachment ambiguity that we examined earlier with phrase structure grammar:"]},{"cell_type":"code","metadata":{"id":"2iWElxl8edwx","colab_type":"code","outputId":"247cabb0-f4da-4230-94ee-140a5fe11af6","executionInfo":{"status":"ok","timestamp":1562916966553,"user_tz":-180,"elapsed":895,"user":{"displayName":"Damla Nisa CEVIK","photoUrl":"","userId":"15647447958135404493"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["pdp = nltk.ProjectiveDependencyParser(groucho_dep_grammar)\n","sent = 'I shot an elephant in my pajamas'.split()\n","trees = pdp.parse(sent)\n","for tree in trees:\n","  print(tree)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(shot I (elephant an (in (pajamas my))))\n","(shot I (elephant an) (in (pajamas my)))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WLC58tHmvYH5","colab_type":"text"},"source":["These bracketed dependency structures can also be displayed as trees, where dependents are shown as children of their heads.\n","> ![alt text](https://i.hizliresim.com/kMnNM7.png)\n","\n","\n","In languages with more flexible word order than English, non-projective dependencies are more frequent.\n","\n","Various criteria have been proposed for deciding what is the head *H* and what is the dependent *D* in a construction *C*. Some of the most important are the following:\n","> 1. *H* determines the distribution class of *C*; or alternatively, the external syntactic properties of *C* are due to *H*.\n","> 2. *H* determines the semantic type of *C*.\n","> 3. *H* is obligatory while *D* may be optional.\n","> 4. *H* selects *D* and determines whether it is obligatory or optional.\n","> 5. The morphological form of *D* is determined by *H* (e.g. agreement or case government).\n","\n","When we say in a phrase structure grammar that the immediate constituents of a `PP` are `P` and `NP`, we are implicitly appealing to the head / dependent distinction. A prepositional phrase is a phrase whose head is a preposition; moreover, the `NP` is a dependent of `P`. The same distinction carries over to the other types of phrase that we have discussed. The key point to note here is that although phrase structure grammars seem very different from dependency grammars, they implicitly embody a recognition of dependency relations. While CFGs are not intended to directly capture dependencies, more recent linguistic frameworks have increasingly adopted formalisms which combine aspects of both approaches."]},{"cell_type":"markdown","metadata":{"id":"OZ0uC3V7we-T","colab_type":"text"},"source":["<h3>Valency and the Lexicon</h3>"]},{"cell_type":"markdown","metadata":{"id":"Le5mi22Gwe7r","colab_type":"text"},"source":["Let us take a closer look at verbs and their dependents. The grammar in the example at 3rd section correctly generates examples like 1d.\n","\n","> 1a.\t\tThe squirrel was frightened.\n","\n","> 1b.\t\tChatterer saw the bear.\n","\n","> 1c.\t\tChatterer thought Buster was angry.\n","\n","> 1d.\t\tJoe put the fish on the log.\n","\n","These possibilities correspond to the following productions:\n","\n","> ![alt text](https://i.hizliresim.com/dLkyEV.png)\n","\n","That is, was can occur with a following `**Adj**`, *saw* can occur with a following `NP`, *thought* can occur with a following `S` and *put* can occur with a following `NP` and `PP`. The dependents `Adj`, `NP`, `PP` and `S` are often called **complements** of the respective verbs and there are strong constraints on what verbs can occur with what complements.\n","\n","By contrast with 1d, the word sequences in 2d are ill-formed:\n","> 2a.\t\tThe squirrel was Buster was angry.\n","\n","> 2b.\t\tChatterer saw frightened.\n","\n","> 2c.\t\tChatterer thought the bear.\n","\n","> 2d.\t\tJoe put on the log.\n","\n","\n","In the tradition of dependency grammar, the verbs in first example are said to have different **valencies**. Valency restrictions are not just applicable to verbs, but also to the other classes of heads.\n","\n","Within frameworks based on phrase structure grammar, various techniques have been proposed for excluding the ungrammatical examples in the second example. In a CFG, we need some way of constraining grammar productions which expand `VP` so that verbs *only* co-occur with their correct complements. We can do this by dividing the class of verbs into \"subcategories\", each of which is associated with a different set of complements. For example, **transitive verbs** such as *chased* and *saw* require a following `NP` object complement; that is, they are **subcategorized** for `NP` direct objects. If we introduce a new category label for transitive verbs, namely `TV` (for Transitive Verb), then we can use it in the following productions:\n","\n","\n","\n","```\n","VP -> TV NP\n","TV -> 'chased' | 'saw'\n","```\n","\n","> ![alt text](https://i.hizliresim.com/9Y2JBN.png)"]},{"cell_type":"markdown","metadata":{"id":"RIE7tftIz19-","colab_type":"text"},"source":["Complements are often contrasted with modifiers (or adjuncts), although both are kinds of dependent. Prepositional phrases, adjectives and adverbs typically function as modifiers. Unlike complements, modifiers are optional, can often be iterated, and are not selected for by heads in the same way as complements. For example, the adverb *really* can be added as a modifer to all the sentence in 3d:\n","\n","> 3a. The squirrel really was frightened.\n","\n","> 3b. Chatterer really saw the bear.\n","\n","> 3c. \tChatterer really thought Buster was angry.\n","\n","> 3d. Joe really put the fish on the log.\n","\n","The structural ambiguity of `PP` attachment, which we have illustrated in both phrase structure and dependency grammars, corresponds semantically to an ambiguity in the scope of the modifier."]},{"cell_type":"markdown","metadata":{"id":"qsNrxHdO0OxM","colab_type":"text"},"source":["<h2>6. Grammar Development</h2>"]},{"cell_type":"markdown","metadata":{"id":"0piWUnIF0Y95","colab_type":"text"},"source":["<h3>Treebanks and Grammars</h3>\n","\n","The `corpus` module defines the `treebank` corpus reader, which contains a 10% sample of the Penn Treebank corpus."]},{"cell_type":"code","metadata":{"id":"6Cmt3dG10gab","colab_type":"code","outputId":"c3fb28f1-16c3-4897-b097-1cc85330653e","executionInfo":{"status":"ok","timestamp":1562916998220,"user_tz":-180,"elapsed":636,"user":{"displayName":"Damla Nisa CEVIK","photoUrl":"","userId":"15647447958135404493"}},"colab":{"base_uri":"https://localhost:8080/","height":260}},"source":["from nltk.corpus import treebank\n","t = treebank.parsed_sents('wsj_0001.mrg')[0]\n","print(t)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(S\n","  (NP-SBJ\n","    (NP (NNP Pierre) (NNP Vinken))\n","    (, ,)\n","    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n","    (, ,))\n","  (VP\n","    (MD will)\n","    (VP\n","      (VB join)\n","      (NP (DT the) (NN board))\n","      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n","      (NP-TMP (NNP Nov.) (CD 29))))\n","  (. .))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M_WAnJbP0Y7n","colab_type":"text"},"source":["We can use this data to help develop a grammar. For example, the program in below uses a simple filter to find verbs that take sentential complements. Assuming we already have a production of the form `VP` -> `Vs S`, this information enables us to identify particular verbs that would be included in the expansion of `Vs`."]},{"cell_type":"code","metadata":{"id":"QFse0-OIGltH","colab_type":"code","colab":{}},"source":["def filter(tree):\n","    child_nodes = [child.label() for child in tree\n","                   if isinstance(child, nltk.Tree)]\n","    return  (tree.label() == 'VP') and ('S' in child_nodes)\n","  \n","from nltk.corpus import treebank\n","[subtree for tree in treebank.parsed_sents()\n","  for subtree in tree.subtrees(filter)]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b4hNoeEu0Y43","colab_type":"text"},"source":["The Prepositional Phrase Attachment Corpus, `nltk.corpus.ppattach` is another source of information about the valency of particular verbs. Here we illustrate a technique for mining this corpus. It finds pairs of prepositional phrases where the preposition and noun are fixed, but where the choice of verb determines whether the prepositional phrase is attached to the `VP` or to the `NP`.\n","\n","Amongst the output lines of this program we find offer-from-group N: ['rejected'] V: ['received'], which indicates that *received* expects a separate `PP` complement attached to the `VP`, while rejected does not. As before, we can use this information to help construct the grammar."]},{"cell_type":"code","metadata":{"id":"_9wiy4DPvT2A","colab_type":"code","colab":{}},"source":["from collections import defaultdict\n","entries = nltk.corpus.ppattach.attachments('training')\n","table = defaultdict(lambda: defaultdict(set))\n","for entry in entries:\n","  key = entry.noun1 + '-' + entry.prep + '-' + entry.noun2\n","  table[key][entry.attachment].add(entry.verb)\n","\n","for key in sorted(table):\n","  if len(table[key]) > 1:\n","    print(key, 'N:', sorted(table[key]['N']), 'V:', sorted(table[key]['V']))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kJ32sGdmIPlc","colab_type":"text"},"source":["The NLTK corpus collection includes data from the PE08 Cross-Framework and Cross Domain Parser Evaluation Shared Task. A collection of larger grammars has been prepared for the purpose of comparing different parsers, which can be obtained by downloading the `large_grammars` package (e.g. `python -m nltk.downloader large_grammars`).\n","\n","The NLTK corpus collection also includes a sample from the *Sinica Treebank Corpus*, consisting of 10,000 parsed sentences drawn from the *Academia Sinica Balanced Corpus of Modern Chinese*. If you want to display one of the trees in this corpus switch to different environment, since Colaboratory is not supporting displays. You should write the code below:\n","\n","\n","```\n","nltk.corpus.sinica_treebank.parsed_sents()[3450].draw()              \n","```\n","\n","The output:\n","\n","\n","![alt text](https://i.hizliresim.com/Z5kEgk.png)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hhlj6RmHJZGu","colab_type":"text"},"source":["<h3>Pernicious Ambiguity</h3>"]},{"cell_type":"markdown","metadata":{"id":"AY3rcAMXJZDf","colab_type":"text"},"source":["Unfortunately, as the coverage of the grammar increases and the length of the input sentences grows, the number of parse trees grows rapidly. In fact, it grows at an astronomical rate.\n","\n","Let's explore this issue with the help of a simple example. The word *fish* is both a noun and a verb. We can make up the sentence *fish fish fish*, meaning* fish like to fish for other fish*. (Try this with *police* if you prefer something more sensible.) Here is a toy grammar for the \"fish\" sentences."]},{"cell_type":"code","metadata":{"id":"CJPfGEvUIzd2","colab_type":"code","colab":{}},"source":["grammar = nltk.CFG.fromstring(\"\"\"\n","    S -> NP V NP\n","    NP -> NP Sbar\n","    Sbar -> NP V\n","    NP -> 'fish'\n","    V -> 'fish'\n","    \"\"\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xh9x9TZZKMV-","colab_type":"text"},"source":["Now we can try parsing a longer sentence, *fish fish fish fish fish*, which amongst other things, means '*fish that other fish fish are in the habit of fishing fish themselves*'. We use the NLTK chart parser, which was mentioned earlier in this chapter. This sentence has two readings."]},{"cell_type":"code","metadata":{"id":"9XXXj8c3KI2f","colab_type":"code","outputId":"dbb2ba1c-e966-4804-84b5-37516c8cacc8","executionInfo":{"status":"ok","timestamp":1562917830762,"user_tz":-180,"elapsed":698,"user":{"displayName":"Damla Nisa CEVIK","photoUrl":"","userId":"15647447958135404493"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["tokens = [\"fish\"] * 5\n","cp = nltk.ChartParser(grammar)\n","for tree in cp.parse(tokens):\n","  print(tree)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(S (NP fish) (V fish) (NP (NP fish) (Sbar (NP fish) (V fish))))\n","(S (NP (NP fish) (Sbar (NP fish) (V fish))) (V fish) (NP fish))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fVjbhT8hKhZu","colab_type":"text"},"source":["<h3>Weighted Grammar</h3>"]},{"cell_type":"markdown","metadata":{"id":"7YGpNIQpKhWc","colab_type":"text"},"source":["Chart parsers improve the efficiency of computing multiple parses of the same sentences, but they are still overwhelmed by the sheer number of possible parses. Weighted grammars and probabilistic parsing algorithms have provided an effective solution to these problems.\n","\n","Before looking at these, we need to understand why the notion of grammaticality could be *gradient*. Considering the verb *give*. This verb requires both a direct object (the thing being given) and an indirect object (the recipient). These complements can be given in either order, as illustrated in 1. In the \"prepositional dative\" form in 1a, the direct object appears first, followed by a prepositional phrase containing the indirect object.\n","\n","> 1a. Kim gave a bone to the dog\n","\n","> 1b. Kim gave the dog a bone\n","\n","In the \"double object\" form in 1b, the indirect object appears first, followed by the direct object. In the above case, either order is acceptable. However, if the indirect object is a pronoun, there is a strong preference for the double object construction:\n","\n","> 2a. Kim gives the heebie-jeebies to me (*prepositional dative)\n","\n","> 2b. Kim gives me the heebie-jeebies (double object)\n","\n","Using the Penn Treebank sample, we can examine all instances of prepositional dative and double object constructions involving give:\n","\n"]},{"cell_type":"code","metadata":{"id":"T6KV53nnPZO9","colab_type":"code","colab":{}},"source":["def give(t):\n","    return t.label() == 'VP' and len(t) > 2 and t[1].label() == 'NP'\\\n","           and (t[2].label() == 'PP-DTV' or t[2].label() == 'NP')\\\n","           and ('give' in t[0].leaves() or 'gave' in t[0].leaves())\n","  \n","def sent(t):\n","    return ' '.join(token for token in t.leaves() if token[0] not in '*-0')\n","  \n","def print_node(t, width):\n","        output = \"%s %s: %s / %s: %s\" %\\\n","            (sent(t[0]), t[1].label(), sent(t[1]), t[2].label(), sent(t[2]))\n","        if len(output) > width:\n","            output = output[:width] + \"...\"\n","        print(output)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vaaF-vSzPa4D","colab_type":"code","colab":{}},"source":["for tree in nltk.corpus.treebank.parsed_sents():\n","  for t in tree.subtrees(give):\n","    print_node(t, 72)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7bwBNdSyKhS5","colab_type":"text"},"source":["We can observe a strong tendency for the shortest complement to appear first. However, this does not account for a form like `give NP: federal judges / NP`: a raise, where animacy may play a role. In fact there turn out to be a large number of contributing factors, as surveyed by (Bresnan & Hay, 2006). Such preferences can be represented in a weighted grammar.\n","\n","A **probabilistic context free grammar** (or PCFG) is a context free grammar that associates a probability with each of its productions. It generates the same set of parses for a text that the corresponding context free grammar does, and assigns a probability to each parse. The probability of a parse generated by a PCFG is simply the product of the probabilities of the productions used to generate it.\n","\n","The simplest way to define a PCFG is to load it from a specially formatted string consisting of a sequence of weighted productions, where weights appear in brackets:"]},{"cell_type":"code","metadata":{"id":"gCjL_iLZKX0d","colab_type":"code","colab":{}},"source":["grammar = nltk.PCFG.fromstring(\"\"\"\n","    S    -> NP VP              [1.0]\n","    VP   -> TV NP              [0.4]\n","    VP   -> IV                 [0.3]\n","    VP   -> DatV NP NP         [0.3]\n","    TV   -> 'saw'              [1.0]\n","    IV   -> 'ate'              [1.0]\n","    DatV -> 'gave'             [1.0]\n","    NP   -> 'telescopes'       [0.8]\n","    NP   -> 'Jack'             [0.2]\n","    \"\"\")\n","\n","print(grammar)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9c0vnQ0hQRlo","colab_type":"text"},"source":["It is sometimes convenient to combine multiple productions into a single line, e.g. `VP -> TV NP [0.4] | IV [0.3] | DatV NP NP [0.3]`. In order to ensure that the trees generated by the grammar form a probability distribution, PCFG grammars impose the constraint that all productions with a given left-hand side must have probabilities that sum to one. The grammar in above example obeys this constraint: for S, there is only one production, with a probability of 1.0; for VP, 0.4+0.3+0.3=1.0; and for NP, 0.8+0.2=1.0. The parse tree returned by parse() includes probabilities:"]},{"cell_type":"code","metadata":{"id":"pSsF8SifQI-0","colab_type":"code","outputId":"45ca49b5-e31b-4097-ee53-29e03da45bf0","executionInfo":{"status":"ok","timestamp":1562919434379,"user_tz":-180,"elapsed":625,"user":{"displayName":"Damla Nisa CEVIK","photoUrl":"","userId":"15647447958135404493"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["viterbi_parser = nltk.ViterbiParser(grammar)\n","for tree in viterbi_parser.parse(['Jack', 'saw', 'telescopes']):\n","  print(tree)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(S (NP Jack) (VP (TV saw) (NP telescopes))) (p=0.064)\n"],"name":"stdout"}]}]}